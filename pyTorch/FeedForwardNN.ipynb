{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c683b9f4",
   "metadata": {},
   "source": [
    "# Simple Neural Network Template\n",
    "Goal of this notebook is to avoid rewrinting code for Neural Networks with tabular data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "552b3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2f5e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b9f1c2",
   "metadata": {},
   "source": [
    "# Prepare The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd88057",
   "metadata": {},
   "source": [
    "## Data Loading and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5dd7e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>68</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>58</td>\n",
       "      <td>52</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>58</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1  f2  f3   y\n",
       "0  30  29  68  89\n",
       "1   5  51  14  95\n",
       "2   6   1   9  65\n",
       "3  30  58  52  58\n",
       "4  72  58  72  72"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a random df instead of df = pd.read_csv(\"\") \n",
    "df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=[\"f1\",\"f2\",\"f3\",\"y\"])\n",
    "\n",
    "# list of columns used as features and as target\n",
    "feaures = [\"f1\",\"f2\",\"f3\"]\n",
    "target = [\"y\"]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b1e7982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training feature set:\t (70, 3)\n",
      "Shape of the training target set:\t (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# if needed, spit the data into train\\validation and test set\n",
    "features = df[feaures].values\n",
    "targets = df[target].values\n",
    "\n",
    "# Split the dataset into training and temp sets (85% train, 15% temp)\n",
    "features_train, features_temp, targets_train, targets_temp = train_test_split(\n",
    "    features, targets, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "# Further split the temp set into validation and test sets \n",
    "features_val, features_test, targets_val, targets_test = train_test_split(\n",
    "    features_temp, targets_temp, test_size=0.5, random_state=0\n",
    ")\n",
    "\n",
    "# Note that from now we are using numpy array of dimension (sample, features) \n",
    "print(f\"Shape of the training feature set:\\t {features_train.shape}\")\n",
    "print(f\"Shape of the training target set:\\t {targets_temp.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac5adb",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d040449",
   "metadata": {},
   "source": [
    "Scaling is important for NN traing:\n",
    "- NN are trained with gradient-based optimization. If input features have very different ranges (e.g., one is in [0,1], another in [0,1000]), the gradients become unbalanced\n",
    "- Weights are usually initialized with small random values. If inputs vary a lot in scale, some neurons saturate (e.g., sigmoid stuck at 0 or 1), killing gradients.\n",
    "- A feature with large values might artificially look more important, skewing learning.\n",
    "\n",
    "Here the most common methods from sklearn: \n",
    "| Method         | Formula                  | Best For                        | Sensitive to Outliers |\n",
    "|----------------|--------------------------|---------------------------------|-----------------------|\n",
    "| StandardScaler | (x - μ) / σ             | General NN input, Gaussian data |  Yes                |\n",
    "| MinMaxScaler   | (x - min) / (max - min) | Images, bounded features        |  Yes                |\n",
    "| RobustScaler   | (x - median) / IQR      | Skewed data, outliers present   |  No                 |\n",
    "| MaxAbsScaler   | x / max(abs(x))         | Sparse data, [-1,1] scaling     |  Yes                |\n",
    "\n",
    "For sake of semplicity:\n",
    "\n",
    "- Images → MinMax (0–1)\n",
    "- General tabular → StandardScaler\n",
    "- Outlier-heavy data → RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0aa0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# remember to fit only of training data\n",
    "scaler.fit(features_train) \n",
    "\n",
    "# Transform the training, validation, and test sets\n",
    "features_train = scaler.transform(features_train)\n",
    "features_val = scaler.transform(features_val)\n",
    "features_test = scaler.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e731e2b",
   "metadata": {},
   "source": [
    "## DataSet & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979d028",
   "metadata": {},
   "source": [
    "In PyTorch, data handling is split into two complementary abstractions. A Dataset defines what the data is and how to access a single sample, while a DataLoader defines how to efficiently serve that data to a model in batches. This separation allows you to keep the logic of accessing data independent from the logic of batching, shuffling, and parallelizing.\n",
    "\n",
    "**Dataset**: defines the data access pattern.\n",
    "- Implements __getitem__(index) → returns a single (features, target) pair.\n",
    "- Implements __len__() → reports dataset size (for map-style datasets).\n",
    "- Can be map-style (random access) or iterable (streaming).\n",
    "\n",
    "**DataLoader**: orchestrates data delivery.\n",
    "- Wraps a Dataset and produces mini-batches.\n",
    "- Handles shuffling, parallel sample loading (num_workers), and batch collation into tensors.\n",
    "- Provides an iterator over batches, enabling efficient training loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8dd8d2",
   "metadata": {},
   "source": [
    "For tabular data, we have 2 possible options for the creation of the dataset.\n",
    "\n",
    "\n",
    "* **Custom Dataset** ([docs](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html))\n",
    "    - subclass `torch.utils.data.Dataset` and must implement `__len__` / `__getitem__`.\n",
    "    -  Flexible: supports preprocessing (scaling, encoding, imputation), lazy loading from disk, and any custom logic for accessing samples.\n",
    "    - Needed for non-ram fitting dataset\n",
    "* **`TensorDataset`** – lightweight wrapper around `(X_tensor, y_tensor)` when data is already preprocessed and fits in memory. \n",
    "    -  Minimal boilerplate, but no support for on-the-fly transforms or complex loading.\n",
    "\n",
    "\n",
    "Would you like me to now provide **minimal code examples** for both in the same format (so you can compare side by side)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e659575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.Tensor(features_train), torch.Tensor(targets_train))\n",
    "val_dataset = TensorDataset(torch.Tensor(features_val), torch.Tensor(targets_val))\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# Create DataLoaders for training, validation, and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b40c4",
   "metadata": {},
   "source": [
    "# Your Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec5115",
   "metadata": {},
   "source": [
    "## Define It"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d7554",
   "metadata": {},
   "source": [
    "When creating a custom neural network by subclassing `nn.Module`, the class typically includes **required components** and **optional but common patterns**.\n",
    "\n",
    "### Required Components\n",
    "- **`__init__(self, ...)`**  \n",
    "  - Purpose: define all layers and components of the network.  \n",
    "  - Must call `super().__init__()` at the beginning.  \n",
    "  - Typical layers: `nn.Linear`, `nn.Conv2d`, `nn.ReLU`, `nn.BatchNorm1d`, `nn.Dropout`.\n",
    "\n",
    "- **`forward(self, x)`**  \n",
    "  - Purpose: defines the computation of the network, i.e., how the input `x` is transformed into the output.  \n",
    "  - Must return a tensor.  \n",
    "  - **Do not call this directly**; PyTorch internally calls it when you execute `model(input)`.\n",
    "\n",
    "\n",
    ")Pay Attention): In theory, the last layer of a neural network should include an activation function during inference to produce probabilities or bounded outputs. However, most PyTorch loss functions (e.g., `BCEWithLogitsLoss`, `CrossEntropyLoss`) apply these activations internally for numerical stability and efficiency. Therefore, when defining the network for training, the final activation should not be included in the model. It can then be applied separately during inference.\n",
    "\n",
    "> **Note:** Sometimes tutorials or projects define an `inference` or `predict` method. This is **not required** by PyTorch; it’s just a convenience wrapper that calls `forward` (often with `torch.no_grad()` for evaluation) to separate training vs prediction logic.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Possible Improvements\n",
    "\n",
    "- **Deeper / Wider Layers**: increase representational capacity.  \n",
    "- **Regularization**: dropout and batch normalization for stability and generalization.  \n",
    "- **Custom Activations**: ReLU, LeakyReLU, GELU, SiLU.  \n",
    "- **Residual Connections**: optionally add transformed inputs to intermediate layers.  \n",
    "- **Weight Initialization**: e.g., Kaiming for ReLU, Xavier for Sigmoid/Tanh.  \n",
    "- **Output Layer**: adapt with Sigmoid/Softmax depending on task.  \n",
    "- **Optimizer & Scheduler**: Adam, SGD+momentum, learning rate scheduling.  \n",
    "- **Input Preprocessing**: standardization, normalization, categorical encoding for tabular data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b143ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim,drop_out_par):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Flatten input\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Optional residual: projects input to match first hidden layer\n",
    "        self.residual = nn.Linear(input_dim, 256)\n",
    "\n",
    "        # Feature extraction\n",
    "        self.features_extraction = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),                          # Can switch to LeakyReLU, GELU, SiLU\n",
    "            nn.BatchNorm1d(64),                 # Optional batch normalization\n",
    "            nn.Dropout(drop_out_par),           # Optional dropout for regularization\n",
    "\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(drop_out_par),\n",
    "\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(drop_out_par)\n",
    "        )\n",
    "\n",
    "        # Head (output)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 1) \n",
    "            # (Pay Attention): nn.Sigmoid() or nn.Softmax(dim=1) depending on task (Replace 1 with num_classes for multi-class)\n",
    "         ) \n",
    "        \n",
    "        # Weight initialization hint\n",
    "        # nn.init.kaiming_uniform_(self.features_extraction[0].weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Residual connection: add projected input to first hidden layer output\n",
    "        res = self.residual(x)\n",
    "        features = self.features_extraction(x)\n",
    "        features = features + res  # simple residual addition\n",
    "\n",
    "        # Output\n",
    "        logits = self.head(features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bccca72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NeuralNetwork                            [1, 1]                    --\n",
       "├─Flatten: 1-1                           [1, 3]                    --\n",
       "├─Linear: 1-2                            [1, 256]                  1,024\n",
       "├─Sequential: 1-3                        [1, 256]                  --\n",
       "│    └─Linear: 2-1                       [1, 64]                   256\n",
       "│    └─ReLU: 2-2                         [1, 64]                   --\n",
       "│    └─BatchNorm1d: 2-3                  [1, 64]                   128\n",
       "│    └─Dropout: 2-4                      [1, 64]                   --\n",
       "│    └─Linear: 2-5                       [1, 128]                  8,320\n",
       "│    └─ReLU: 2-6                         [1, 128]                  --\n",
       "│    └─BatchNorm1d: 2-7                  [1, 128]                  256\n",
       "│    └─Dropout: 2-8                      [1, 128]                  --\n",
       "│    └─Linear: 2-9                       [1, 256]                  33,024\n",
       "│    └─ReLU: 2-10                        [1, 256]                  --\n",
       "│    └─BatchNorm1d: 2-11                 [1, 256]                  512\n",
       "│    └─Dropout: 2-12                     [1, 256]                  --\n",
       "├─Sequential: 1-4                        [1, 1]                    --\n",
       "│    └─Linear: 2-13                      [1, 1]                    257\n",
       "==========================================================================================\n",
       "Total params: 43,777\n",
       "Trainable params: 43,777\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.04\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.18\n",
       "Estimated Total Size (MB): 0.18\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember that the features sets are composed by (n_sample,n_features). So a sample is a vector of n_featues elements. \n",
    "input_dim = features_train.shape[1]\n",
    "drop_out_par = 0.1\n",
    "\n",
    "model = NeuralNetwork(input_dim,drop_out_par)\n",
    "summary(model, input_size=(1, input_dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf5e39",
   "metadata": {},
   "source": [
    "## Train It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7761aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = \n",
    "optimizer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae00b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for batch, (X, y) in enumerate(train_dataset):\n",
    "\n",
    "    # Compute prediction and loss\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        loss, current = loss.item(), batch * batch_size + len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyBetting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
